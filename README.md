# Multi-LLM Idea Discussion App

This project is a web application that facilitates a simulated discussion between multiple Large Language Models (LLMs) based on user-defined topics and personas. It leverages various LLMs through the Groq platform to provide a diverse range of perspectives.

## Directory Structure
```
multi-llm-idea-discussion-app/
├── README.md
├── LICENSE
├── Road_ahead.md
├── requirements.txt
├── backend/
│   ├── main.py
│   ├── models/
│   │   ├── dee_see.py
│   │   ├── dee_see_meta.py
│   │   ├── gemma_model.py
│   │   ├── llama_model_1.py
│   │   └── middle_llama.py
│   └── services/
│       └── event_summarizer.py
└── frontend/
    ├── home.html
    ├── scripts.js
    └── styles.css
```
## Requirements
```
langchain-groq
streamlit
python-dotenv
fastapi
uvicorn
pydantic
```
You can install these dependencies using pip:
```
pip install -r requirements.txt
```
## Backend (FastAPI)

The backend is built using FastAPI and serves as the API for the application.

### main.py

- Sets up a FastAPI application with CORS middleware.
- Loads environment variables, including the Groq API key.
- Defines a DataModel for input data (topic, names, and professions).
- Implements endpoints:
    - /send_data/: Receives input data and initializes the discussion.
    - /discussion/: Streams the discussion generated by various LLMs.
- Integrates different LLM models from the models directory.

### models/

- dee_see.py: Uses the deepseek-r1-distill-qwen-32b model.
- dee_see_meta.py: Uses the deepseek-r1-distill-llama-70b model.
- gemma_model.py: Uses the qwen-2.5-32b model.
- llama_model_1.py: Uses the llama-3.1-8b-instant model.
- middle_llama.py: Uses the llama-3.2-3b-preview model.
- Each model file defines a function to generate responses based on the provided context.

### services/

- event_summarizer.py: Uses the llama-3.3-70b-versatile model to summarize the discussion.

## Frontend (HTML, CSS, JavaScript)

The frontend provides a user interface for entering discussion parameters and displaying the generated discussion.

### home.html

- Contains the HTML structure for the application.
- Includes a form to input the topic, names, and professions.
- Displays the discussion in the right container.

### scripts.js

- Handles form submission and sends data to the backend.
- Uses Server-Sent Events (SSE) to stream the discussion in real-time.
- Implements a scrolling animation and a text streaming effect for the header.

### styles.css

- Provides styling for the application.
- Includes responsive design for various screen sizes.

## How to Run

1. Install Dependencies:
```
pip install -r requirements.txt
```
2. Set Up Environment Variables:

- Create a .env file in the root directory.
- Add your Groq API key: GROQ_API_KEY=your_api_key.
- if you dont want to use .env file you can also enter the key on terminal when backend starts.

3. Run the Backend:
```
uvicorn backend.main:app --reload
```
4. Open the Frontend:

- Open frontend/home.html in your web browser.

5. Use the Application:

- Enter the topic, names, and professions in the form.
- Click "Submit" to start the discussion.

## License

This project is licensed under the LICENSE file.

## Road Ahead

See Road_ahead.md for future development plans.
